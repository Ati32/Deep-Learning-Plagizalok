{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "milestone2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4cYv7iGL70K",
        "outputId": "39e152c6-b4ce-46b8-8d91-b2b976135cbe"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "#from tensorflow.random import set_seed\n",
        "#set_seed(2)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import pickle\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "#np.random.seed(1)\n",
        "random_state = 7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZDZEeviL-Bp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc79aef-c2fb-4215-a738-1683101e5ed2"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#train = pd.read_csv('/content/drive/My Drive/Author_identification/train.csv')\n",
        "#test = pd.read_csv('/content/drive/My Drive/Author_identification/test.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74_xgZ9B9qr6"
      },
      "source": [
        "train = pd.read_csv('./Data/Author_identification/train.csv')\n",
        "test = pd.read_csv('./Data/Author_identification/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GY_GOKilL_h7",
        "outputId": "7e3a7d5d-6d9e-4476-ac36-ee95ea67cedb"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text author\n",
              "0  id26305  This process, however, afforded me no means of...    EAP\n",
              "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
              "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
              "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "D3Xp8ZVOMCht",
        "outputId": "b4f8415e-bed1-4bca-f9fd-86b62e0412c7"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id02310</td>\n",
              "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id24541</td>\n",
              "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id00134</td>\n",
              "      <td>And when they had broken down the frail door t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27757</td>\n",
              "      <td>While I was thinking how I should possibly man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id04081</td>\n",
              "      <td>I am not sure to what limit his knowledge may ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text\n",
              "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
              "1  id24541  If a fire wanted fanning, it could readily be ...\n",
              "2  id00134  And when they had broken down the frail door t...\n",
              "3  id27757  While I was thinking how I should possibly man...\n",
              "4  id04081  I am not sure to what limit his knowledge may ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "9wMnQrp5MOUi",
        "outputId": "f212e236-ff00-4b40-925c-d7ff696f4827"
      },
      "source": [
        "train['sentences'] = train.text.transform(lambda x: len(sent_tokenize(x)))\n",
        "train['words'] = train.text.transform(lambda x: len(word_tokenize(x)))\n",
        "train['text_length'] = train.text.transform(lambda x: len(x))\n",
        "\n",
        "text_info = train.groupby(\"author\")[['sentences','words','text_length']].sum()\n",
        "text_info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>words</th>\n",
              "      <th>text_length</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>author</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>EAP</th>\n",
              "      <td>8206</td>\n",
              "      <td>232184</td>\n",
              "      <td>1123585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HPL</th>\n",
              "      <td>5876</td>\n",
              "      <td>173979</td>\n",
              "      <td>878178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MWS</th>\n",
              "      <td>6128</td>\n",
              "      <td>188824</td>\n",
              "      <td>916632</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        sentences   words  text_length\n",
              "author                                \n",
              "EAP          8206  232184      1123585\n",
              "HPL          5876  173979       878178\n",
              "MWS          6128  188824       916632"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0pxgbPuMPqf"
      },
      "source": [
        "stop_words = set(stopwords.words('english')).union(set([',','.','?','!',':',';',\"'\",'\"','-',\"''\",\"`\",\"``\"]))\n",
        "author_dict = {'EAP': 0, 'HPL': 1, 'MWS': 2}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQhh68p2MTOJ"
      },
      "source": [
        "new_text = []\n",
        "new_author = []\n",
        "for i, row in train[['text','author']].iterrows():\n",
        "    word_tokens = word_tokenize(row['text'])\n",
        "    new_text.append(\" \".join([w for w in word_tokens if not w in stop_words]))\n",
        "    new_author.append(author_dict[row['author']])\n",
        "new_train = pd.DataFrame(data={'text': new_text, 'author': new_author})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fle-tbTTMTjO"
      },
      "source": [
        "new_test_text = []\n",
        "for i, row in test[['text']].iterrows():\n",
        "    word_tokens = word_tokenize(row['text'])\n",
        "    new_test_text.append(\" \".join([w for w in word_tokens if not w in stop_words]))\n",
        "new_test = pd.DataFrame(data={'text': new_text})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ucX1DFgMWuJ"
      },
      "source": [
        "X = new_train['text']\n",
        "y = new_train['author']\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
        "X_test = new_test['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg4aCFDbMZTZ"
      },
      "source": [
        "new_text_ws = [] # without stopwords\n",
        "for i, row in train[['text']].iterrows():\n",
        "    word_tokens = word_tokenize(row['text'])\n",
        "    new_text_ws.append(\" \".join([w for w in word_tokens if not w in [',','.','?','!',':',';',\"'\",'\"','-',\"''\",\"`\",\"``\"]]).lower())\n",
        "new_train_ws = pd.DataFrame(data={'text': new_text_ws, 'author': new_author})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgp47saTMa3W"
      },
      "source": [
        "X_ws = new_train_ws['text']\n",
        "y_ws = new_train_ws['author']\n",
        "X_ws_train, X_ws_valid, y_ws_train, y_ws_valid = train_test_split(X_ws, y_ws, test_size=0.3, random_state=random_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ownxaroKMs8i",
        "outputId": "eefa3992-101e-40d5-c725-35b104ba2c1d"
      },
      "source": [
        "X_ws.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    this process however afforded me no means of a...\n",
              "1    it never once occurred to me that the fumbling...\n",
              "2    in his left hand was a gold snuff box from whi...\n",
              "3    how lovely is spring as we looked from windsor...\n",
              "4    finding nothing else not even gold the superin...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "L6FXF4iuMdT0",
        "outputId": "78cb9d8f-63b2-4e6d-aa1a-c89b1b614937"
      },
      "source": [
        "new_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This process however afforded means ascertaini...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It never occurred fumbling might mere mistake</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In left hand gold snuff box capered hill cutti...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How lovely spring As looked Windsor Terrace si...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Finding nothing else even gold Superintendent ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19574</th>\n",
              "      <td>I could fancied I looked eminent landscape pai...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19575</th>\n",
              "      <td>The lids clenched together spasm</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19576</th>\n",
              "      <td>Mais il faut agir say Frenchman never faints o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19577</th>\n",
              "      <td>For item news like strikes us coolly received</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19578</th>\n",
              "      <td>He laid gnarled claw shoulder seemed shaking a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19579 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  author\n",
              "0      This process however afforded means ascertaini...       0\n",
              "1          It never occurred fumbling might mere mistake       1\n",
              "2      In left hand gold snuff box capered hill cutti...       0\n",
              "3      How lovely spring As looked Windsor Terrace si...       2\n",
              "4      Finding nothing else even gold Superintendent ...       1\n",
              "...                                                  ...     ...\n",
              "19574  I could fancied I looked eminent landscape pai...       0\n",
              "19575                   The lids clenched together spasm       0\n",
              "19576  Mais il faut agir say Frenchman never faints o...       0\n",
              "19577      For item news like strikes us coolly received       0\n",
              "19578  He laid gnarled claw shoulder seemed shaking a...       1\n",
              "\n",
              "[19579 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJlYcslBMeph"
      },
      "source": [
        "X_train_list = list(X_train)\n",
        "y_train_list = list(y_train)\n",
        "X_valid_list = list(X_valid)\n",
        "y_valid_list = list(y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvRBrwnhMm9P"
      },
      "source": [
        "X_ws_train_list = list(X_ws_train)\n",
        "y_ws_train_list = list(y_ws_train)\n",
        "X_ws_valid_list = list(X_ws_valid)\n",
        "y_ws_valid_list = list(y_ws_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNXyvrKHXl8t"
      },
      "source": [
        "y_ws_train_list = np.array(y_ws_train_list)\n",
        "y_ws_valid_list = np.array(y_ws_valid_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN1V7uT_RbQr",
        "outputId": "28b1fc18-641b-4e32-e992-d5d2d823ee7b"
      },
      "source": [
        "y_ws_train_list[:50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 1, 2, 0, 1, 1, 2, 0,\n",
              "       0, 1, 2, 2, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Tfu8pugN7OfS",
        "outputId": "6b80c308-7fb1-4c43-8198-62a1bca246b1"
      },
      "source": [
        "X_ws_train_list[0].split()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'should'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK6lkvKJJ4iO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INFDGwRwQeO7"
      },
      "source": [
        "Word2vec vectors for each author"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFX1QzftJ4bB"
      },
      "source": [
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "data_0 = new_train_ws.loc[new_train_ws['author'] == 0]\n",
        "data_1 = new_train_ws.loc[new_train_ws['author'] == 1]\n",
        "data_2 = new_train_ws.loc[new_train_ws['author'] == 2]\n",
        "\n",
        "data_0 = data_0.sample(frac=1).reset_index(drop=True)\n",
        "data_1 = data_1.sample(frac=1).reset_index(drop=True)\n",
        "data_2 = data_2.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "train_0 = data_0[:7400]\n",
        "train_1 = data_1[:5135]\n",
        "train_2 = data_2[:5544]\n",
        "test_0 = data_0[7400:]\n",
        "test_1 = data_1[5135:]\n",
        "test_2 = data_2[5544:]\n",
        "\n",
        "train_0 = data_0[:5000]\n",
        "train_1 = data_1[:5000]\n",
        "train_2 = data_2[:5000]\n",
        "test_0 = data_0[5000:]\n",
        "test_1 = data_1[5000:]\n",
        "test_2 = data_2[5000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-whc2knKDIc",
        "outputId": "1623659f-72ec-443c-b404-15a53c24262c"
      },
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "sent_0 = [row.split() for row in train_0['text']]\n",
        "phrases_0 = Phrases(sent_0, min_count=30, progress_per=10000)\n",
        "bigram_0 = Phraser(phrases_0)\n",
        "sentences_0 = bigram_0[sent_0]\n",
        "w2v_model_0 = Word2Vec(min_count=2, window=2, size=300, sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20, workers=2)\n",
        "\n",
        "from time import time\n",
        "t = time()\n",
        "w2v_model_0.build_vocab(sentences_0, progress_per=10000)\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "t = time()\n",
        "w2v_model_0.train(sentences_0, total_examples=w2v_model_0.corpus_count, epochs=30, report_delay=1)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "w2v_model_0.init_sims(replace=True)\n",
        "\n",
        "w2v_model_0.wv.similarity('you', 'are')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to build vocab: 0.03 mins\n",
            "Time to train the model: 0.34 mins\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.98576355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moeXGhNoKDFg",
        "outputId": "e91d10ee-b7e7-438b-a58d-f6698957a9c6"
      },
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "sent_1 = [row.split() for row in train_1['text']]\n",
        "phrases_1 = Phrases(sent_1, min_count=30, progress_per=10000)\n",
        "bigram_1 = Phraser(phrases_1)\n",
        "sentences_1 = bigram_1[sent_1]\n",
        "w2v_model_1 = Word2Vec(min_count=2, window=2, size=300, sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20, workers=2)\n",
        "\n",
        "from time import time\n",
        "t = time()\n",
        "w2v_model_1.build_vocab(sentences_1, progress_per=10000)\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "t = time()\n",
        "w2v_model_1.train(sentences_1, total_examples=w2v_model_1.corpus_count, epochs=30, report_delay=1)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "w2v_model_1.init_sims(replace=True)\n",
        "\n",
        "w2v_model_1.wv.similarity('you', 'are')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to build vocab: 0.03 mins\n",
            "Time to train the model: 0.39 mins\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9840455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPUnAsnUKF7V",
        "outputId": "547ddeb7-2df2-44fd-9f9a-b7b747738f4f"
      },
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "sent_2 = [row.split() for row in train_2['text']]\n",
        "phrases_2 = Phrases(sent_2, min_count=30, progress_per=10000)\n",
        "bigram_2 = Phraser(phrases_2)\n",
        "sentences_2 = bigram_2[sent_2]\n",
        "w2v_model_2 = Word2Vec(min_count=2, window=2, size=300, sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20, workers=2)\n",
        "\n",
        "from time import time\n",
        "t = time()\n",
        "w2v_model_2.build_vocab(sentences_2, progress_per=10000)\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "t = time()\n",
        "w2v_model_2.train(sentences_2, total_examples=w2v_model_2.corpus_count, epochs=30, report_delay=1)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "w2v_model_2.init_sims(replace=True)\n",
        "\n",
        "w2v_model_2.wv.similarity('you', 'are')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to build vocab: 0.03 mins\n",
            "Time to train the model: 0.37 mins\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.98244613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb9xnUtQKF2C"
      },
      "source": [
        "word_vectors_0 = w2v_model_0.wv\n",
        "word_vectors_1 = w2v_model_1.wv\n",
        "word_vectors_2 = w2v_model_2.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teBbA2ydQsOF"
      },
      "source": [
        "Testing function: \n",
        "- input: test sentence\n",
        "- output: the most likely author"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73p74XVbKKJE"
      },
      "source": [
        "def testing(X,y):\n",
        "  true = 0\n",
        "  false = 0\n",
        "  for index, a in X.items():\n",
        "    #print(a)\n",
        "\n",
        "    sim_0 = 0\n",
        "    sim_1 = 0\n",
        "    sim_2 = 0\n",
        "    sims = []\n",
        "    tokens = word_tokenize(a)\n",
        "    lenght = len(tokens)\n",
        "    for i in range(lenght-1):\n",
        "      #print(i)\n",
        "      if tokens[i] in word_vectors_0 and tokens[i+1] in word_vectors_0:\n",
        "        sim = w2v_model_0.wv.similarity(tokens[i], tokens[i+1])\n",
        "        sim_0 += sim*sim\n",
        "\n",
        "      if tokens[i] in word_vectors_1 and tokens[i+1] in word_vectors_1:\n",
        "        sim = w2v_model_1.wv.similarity(tokens[i], tokens[i+1])\n",
        "        sim_1 += sim*sim\n",
        "\n",
        "      if tokens[i] in word_vectors_2 and tokens[i+1] in word_vectors_2:\n",
        "        sim = w2v_model_2.wv.similarity(tokens[i], tokens[i+1])\n",
        "        sim_2 += sim*sim\n",
        "    \n",
        "    sims.append(sim_0/(lenght-1))\n",
        "    sims.append(sim_1/(lenght-1))\n",
        "    sims.append(sim_2/(lenght-1))\n",
        "    \n",
        "    #print(sims)\n",
        "    #print(np.argmax(sims),y[index])\n",
        "    if np.argmax(sims) == y[index]:\n",
        "      true += 1\n",
        "    else:\n",
        "      false += 1\n",
        "  print(true/(true+false))\n",
        "  return (true, false)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkSIjKJWKKG1",
        "outputId": "9a1b8cab-4bf3-43e3-e491-74c6252f51b8"
      },
      "source": [
        "true_0, false_0 = testing(test_0['text'],test_0['author'])\n",
        "true_1, false_1 = testing(test_1['text'],test_1['author'])\n",
        "true_2, false_2 = testing(test_2['text'],test_2['author'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.7644827586206897\n",
            "0.7307086614173228\n",
            "0.6810344827586207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0QY6apfKKEP",
        "outputId": "a1f57828-aeb8-4a7a-f6e2-0e9c38633766"
      },
      "source": [
        "true = true_0+ true_1 +true_2\n",
        "false = false_0 + false_1 + false_2\n",
        "print(true/(true+false))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7407730945621315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV6TNEttJ4YO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc34HG8RJ4VR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UIHE1aSM4sU"
      },
      "source": [
        "### Pre-trained fastText vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5CFqqwPGiMR"
      },
      "source": [
        "# import gensim.downloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBFIUHRMH50c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ce2cd7-3525-41bb-92c9-0952728efcd6"
      },
      "source": [
        " #fasttext_vectors = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40jVqrbc9qr9"
      },
      "source": [
        "#with open('fasttext_vectors.pickle', 'wb') as f:\n",
        "#    pickle.dump(fasttext_vectors, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFQU7mgH9qr9"
      },
      "source": [
        "with open(\"fasttext_vectors.pickle\", 'rb') as f:\n",
        "    fasttext_vectors = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnYFF947MWoq",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "521c06d9-7785-4881-f7a3-6389c2f28bc8"
      },
      "source": [
        "np.array(fasttext_vectors.wv[X_ws_train_list[0].split()[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.0834e-02, -5.6953e-02, -3.1731e-02,  2.6749e-02,  4.7278e-02,\n",
              "        4.2279e-03,  8.8756e-03, -8.3149e-02, -5.7365e-02, -4.3851e-02,\n",
              "        1.0353e-02,  9.5677e-03,  1.5618e-02, -5.1344e-03,  2.6610e-02,\n",
              "       -2.0032e-03,  1.2627e-01, -3.4053e-02,  7.2836e-02, -2.9336e-02,\n",
              "        1.2728e-02, -5.9716e-02,  4.7661e-02,  8.5972e-02, -3.9190e-02,\n",
              "       -7.3656e-03, -1.8250e-02,  2.8804e-02, -1.9875e-02, -2.4632e-02,\n",
              "       -3.5466e-02, -2.4895e-02,  4.0472e-02,  6.5614e-02,  2.6882e-02,\n",
              "       -8.2732e-02, -9.4424e-03, -2.4687e-02, -4.3293e-02,  1.7315e-02,\n",
              "       -5.1753e-02, -1.6385e-02, -1.0530e-02, -1.6188e-02, -3.0672e-02,\n",
              "       -1.8272e-03, -9.3336e-03, -1.5161e-02,  1.1667e-01, -4.2138e-04,\n",
              "        9.3487e-03, -1.1487e-02, -5.4632e-02, -3.8202e-03, -2.2158e-01,\n",
              "        3.0325e-02,  1.5373e-02,  1.8478e-02, -9.8686e-02, -2.1137e-02,\n",
              "       -1.9092e-02,  1.4057e-02,  1.1103e-01, -2.9714e-02,  4.2807e-02,\n",
              "       -5.1898e-02, -9.7448e-03,  5.6202e-03,  1.5102e-02, -7.2404e-03,\n",
              "        3.9171e-02,  1.0289e-02,  4.9326e-02,  1.1656e-02,  4.3001e-03,\n",
              "       -2.0964e-02, -9.6965e-04, -3.0335e-02,  1.7158e-02,  1.3094e-02,\n",
              "       -4.4167e-02, -7.6330e-03,  2.9762e-02,  1.1258e-01, -4.0120e-02,\n",
              "        3.0029e-02, -2.1949e-02, -2.3894e-02, -3.5168e-02, -1.1499e-02,\n",
              "       -6.6191e-04,  2.9201e-02, -1.0007e-01, -9.1173e-02, -1.1285e-03,\n",
              "       -1.3268e-01, -3.3043e-03, -1.5236e-02, -3.5123e-02,  7.6343e-03,\n",
              "       -3.7059e-02,  2.9431e-03,  2.1348e-02,  2.3537e-03, -4.4561e-02,\n",
              "       -1.4729e-01,  2.7512e-02, -1.1234e-02, -5.5819e-02,  9.8550e-03,\n",
              "        3.4848e-02,  3.5293e-02,  9.8350e-02, -1.9936e-02,  1.9386e-02,\n",
              "       -2.8615e-02, -5.6914e-02,  1.3041e-02, -4.6577e-02,  3.6647e-03,\n",
              "        2.2871e-02, -1.0483e-04,  2.7398e-02, -3.5257e-02,  1.4650e-01,\n",
              "        2.5242e-02, -2.7383e-02, -7.2460e-02, -2.6145e-02,  7.7218e-02,\n",
              "       -1.6321e-02,  6.9845e-03,  1.8923e-02,  4.8347e-02, -6.8252e-02,\n",
              "       -6.5429e-03, -7.0570e-03,  5.6821e-02,  2.5264e-02,  3.5293e-02,\n",
              "        6.5210e-02,  6.7222e-03,  6.4567e-03, -5.5488e-02, -2.7910e-03,\n",
              "       -3.5506e-02,  2.0187e-03,  3.0863e-02, -1.7434e-02,  9.2920e-04,\n",
              "        2.3455e-02,  3.1864e-01, -7.8487e-03, -2.4837e-02,  2.6826e-02,\n",
              "       -2.2413e-02, -2.1342e-02, -2.0674e-02,  1.7606e-02, -1.1007e-01,\n",
              "       -3.0006e-02, -3.3186e-02, -6.1290e-02, -1.7485e-02,  1.5942e-02,\n",
              "       -1.2365e-02,  4.2901e-02, -3.6073e-02,  1.0488e-02,  1.6548e-02,\n",
              "        6.1739e-02, -2.4920e-03,  2.2686e-02, -2.7271e-02,  1.1472e-02,\n",
              "        4.4086e-02, -5.8902e-02,  3.8381e-02,  7.8743e-03,  2.4328e-02,\n",
              "        8.0301e-02,  7.4401e-03, -4.0273e-03,  7.6110e-03,  1.2661e-02,\n",
              "        1.2550e-02,  3.5513e-02,  1.5540e-01, -1.8340e-02,  1.5882e-02,\n",
              "       -8.2935e-03,  2.2966e-01, -1.5243e-01,  1.0406e-02, -3.3330e-02,\n",
              "       -2.2502e-03,  2.4363e-02, -4.7575e-02, -5.5848e-02,  4.6623e-02,\n",
              "       -1.2269e-01,  3.4780e-02,  1.4188e-02,  1.2728e-02, -1.8428e-02,\n",
              "       -4.0353e-02, -2.5896e-02, -2.6679e-03, -4.1411e-02, -1.6941e-01,\n",
              "       -6.6342e-02, -8.0349e-03,  7.2398e-02,  1.3982e-02, -7.1053e-02,\n",
              "       -2.8928e-02, -5.1883e-03, -3.7959e-03,  3.4962e-02, -2.8479e-02,\n",
              "        1.7214e-02, -4.0714e-02, -1.7691e-04,  1.0835e-01, -8.8199e-03,\n",
              "       -1.7168e-03, -2.5204e-02,  7.2845e-02, -5.3177e-03,  1.5578e-03,\n",
              "        1.3101e-02, -1.2352e-01, -3.4532e-02, -9.9064e-03,  4.2388e-02,\n",
              "       -1.6496e-02, -1.4077e-02,  4.2411e-03, -3.8618e-02, -2.1421e-02,\n",
              "        1.7403e-01,  1.0076e-02, -2.2383e-02,  2.1356e-02, -4.1588e-02,\n",
              "       -4.1423e-02,  2.5243e-02, -2.5722e-02, -8.3506e-03, -1.3269e-01,\n",
              "        2.4443e-02,  4.5471e-02, -1.4763e-02,  1.6587e-02,  5.6522e-03,\n",
              "       -5.4695e-02,  1.4901e-02,  2.2947e-02, -5.1056e-02,  2.3629e-02,\n",
              "       -6.4547e-02,  2.4571e-02,  4.1620e-05, -6.4216e-02, -2.1692e-02,\n",
              "        3.1583e-02, -2.9974e-02,  6.9873e-03, -9.9810e-03,  5.5824e-02,\n",
              "        2.1709e-02,  1.0144e-01, -1.5837e-01, -4.0652e-02,  2.3030e-02,\n",
              "        4.2035e-02, -4.3129e-03, -5.5642e-02, -7.0369e-03, -1.8785e-02,\n",
              "       -1.7460e-01, -1.0816e-02,  3.0704e-02, -4.5016e-02,  7.5299e-02,\n",
              "        2.2863e-03, -2.3293e-03,  4.2381e-02, -1.8511e-02, -1.1230e-02,\n",
              "        2.2312e-02, -1.2819e-02, -1.4592e-02, -1.3279e-02, -1.7753e-02,\n",
              "       -3.0646e-02, -1.6548e-02,  3.2178e-02, -4.9309e-02,  3.2872e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0kGXxFMGtLT"
      },
      "source": [
        "#### Dense model with mean vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rksAXxfqIaA5"
      },
      "source": [
        "In each entry we collect the wordvectors of the tokens and take the mean of those (300 dimensional vectors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXHEQpFmM2_V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84764ea9-2bb8-4846-a448-db7024c5e817"
      },
      "source": [
        "X_ws_train_ft_mean_vectors = np.empty([len(X_ws_train_list), 300])\n",
        "for j in range(len(X_ws_train_list)):\n",
        "    X_ws_train_ft_vectors = np.empty([len(X_ws_train_list[j].split()), 300])\n",
        "    for i in range(len(X_ws_train_list[j].split())):\n",
        "        try:\n",
        "            X_ws_train_ft_vectors[i] = fasttext_vectors.wv[X_ws_train_list[j].split()[i]]\n",
        "        except KeyError:\n",
        "            X_ws_train_ft_vectors[i] = np.zeros(300)\n",
        "    X_ws_train_ft_mean_vectors[j] = np.mean(X_ws_train_ft_vectors, axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83m7OK6kS_eB"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X_ws_train_ft_mean_vectors = scaler.fit_transform(X_ws_train_ft_mean_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSNh6pu2SgWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8253b9f-62b3-44f6-baf6-47b13c37ddcf"
      },
      "source": [
        "X_ws_valid_ft_mean_vectors = np.empty([len(X_ws_valid_list), 300])\n",
        "for j in range(len(X_ws_valid_list)):\n",
        "    X_ws_valid_ft_vectors = np.empty([len(X_ws_valid_list[j].split()), 300])\n",
        "    for i in range(len(X_ws_valid_list[j].split())):\n",
        "        try:\n",
        "            X_ws_valid_ft_vectors[i] = fasttext_vectors.wv[X_ws_valid_list[j].split()[i]]\n",
        "        except KeyError:\n",
        "            X_ws_valid_ft_vectors[i] = np.zeros(300)\n",
        "    X_ws_valid_ft_mean_vectors[j] = np.mean(X_ws_valid_ft_vectors, axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vUU--suTAjR"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X_ws_valid_ft_mean_vectors = scaler.fit_transform(X_ws_valid_ft_mean_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlvv28RK7w7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774097d7-2ab3-498a-b37b-9a4af76a0897"
      },
      "source": [
        "len(X_ws_train_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13705"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GXqbkjvi5ZX"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_ws_train_list = to_categorical(y_ws_train_list)\n",
        "y_ws_valid_list = to_categorical(y_ws_valid_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggX1FwKXjPz1",
        "outputId": "54c60cbf-3fe6-4aa7-d932-b680fe54ce0e"
      },
      "source": [
        "y_ws_train_list[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mic7KI7PT3mz",
        "outputId": "e1272c2b-f765-4246-ef9e-71beed0484b5"
      },
      "source": [
        "dense_model = tf.keras.models.Sequential()\n",
        "dense_model.add(tf.keras.Input(shape=(300,)))\n",
        "dense_model.add(tf.keras.layers.Dense(300, activation='relu', kernel_initializer='he_normal'))\n",
        "dense_model.add(Dropout(0.4))\n",
        "dense_model.add(tf.keras.layers.Dense(300, activation='relu', kernel_initializer='he_normal'))\n",
        "dense_model.add(tf.keras.layers.Dense(300, activation='relu', kernel_initializer='he_normal'))\n",
        "dense_model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "dense_model.output_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPZmyS54VlsE",
        "outputId": "29f4a902-26f5-4799-a080-88235a06c701"
      },
      "source": [
        "dense_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 903       \n",
            "=================================================================\n",
            "Total params: 271,803\n",
            "Trainable params: 271,803\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH-iIvnBZcY9"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping=EarlyStopping(monitor=\"val_accuracy\",patience=10, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpGYmq74VvO7"
      },
      "source": [
        "dense_model.compile(optimizer=Adam(lr=1e-3),loss='categorical_crossentropy' ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S42erVs6V40m",
        "outputId": "2740d02c-1818-4815-ac15-b7f32256a824"
      },
      "source": [
        "dense_model.fit(X_ws_train_ft_mean_vectors, y_ws_train_list,  validation_split = 0.2,\n",
        "                     callbacks=[early_stopping], epochs=75)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.9153 - accuracy: 0.6111 - val_loss: 0.7406 - val_accuracy: 0.6830\n",
            "Epoch 2/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.7329 - accuracy: 0.6927 - val_loss: 0.7106 - val_accuracy: 0.7059\n",
            "Epoch 3/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.6633 - accuracy: 0.7242 - val_loss: 0.6955 - val_accuracy: 0.7096\n",
            "Epoch 4/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.6210 - accuracy: 0.7401 - val_loss: 0.6591 - val_accuracy: 0.7231\n",
            "Epoch 5/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.5925 - accuracy: 0.7581 - val_loss: 0.6695 - val_accuracy: 0.7143\n",
            "Epoch 6/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.5552 - accuracy: 0.7715 - val_loss: 0.6572 - val_accuracy: 0.7231\n",
            "Epoch 7/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.5347 - accuracy: 0.7840 - val_loss: 0.6526 - val_accuracy: 0.7289\n",
            "Epoch 8/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4989 - accuracy: 0.7994 - val_loss: 0.6452 - val_accuracy: 0.7424\n",
            "Epoch 9/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4846 - accuracy: 0.8043 - val_loss: 0.6922 - val_accuracy: 0.7231\n",
            "Epoch 10/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4617 - accuracy: 0.8133 - val_loss: 0.6524 - val_accuracy: 0.7402\n",
            "Epoch 11/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4400 - accuracy: 0.8216 - val_loss: 0.6541 - val_accuracy: 0.7402\n",
            "Epoch 12/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4203 - accuracy: 0.8298 - val_loss: 0.6803 - val_accuracy: 0.7344\n",
            "Epoch 13/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3963 - accuracy: 0.8400 - val_loss: 0.6760 - val_accuracy: 0.7457\n",
            "Epoch 14/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3799 - accuracy: 0.8505 - val_loss: 0.7036 - val_accuracy: 0.7344\n",
            "Epoch 15/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3715 - accuracy: 0.8533 - val_loss: 0.7038 - val_accuracy: 0.7329\n",
            "Epoch 16/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3490 - accuracy: 0.8603 - val_loss: 0.6885 - val_accuracy: 0.7308\n",
            "Epoch 17/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3352 - accuracy: 0.8668 - val_loss: 0.7258 - val_accuracy: 0.7184\n",
            "Epoch 18/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3193 - accuracy: 0.8732 - val_loss: 0.7611 - val_accuracy: 0.7366\n",
            "Epoch 19/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3096 - accuracy: 0.8802 - val_loss: 0.7362 - val_accuracy: 0.7384\n",
            "Epoch 20/75\n",
            "343/343 [==============================] - 2s 4ms/step - loss: 0.3044 - accuracy: 0.8823 - val_loss: 0.7647 - val_accuracy: 0.7384\n",
            "Epoch 21/75\n",
            "343/343 [==============================] - 2s 4ms/step - loss: 0.2892 - accuracy: 0.8899 - val_loss: 0.7921 - val_accuracy: 0.7235\n",
            "Epoch 22/75\n",
            "343/343 [==============================] - 2s 4ms/step - loss: 0.2732 - accuracy: 0.8963 - val_loss: 0.7587 - val_accuracy: 0.7322\n",
            "Epoch 23/75\n",
            "343/343 [==============================] - 2s 4ms/step - loss: 0.2673 - accuracy: 0.8961 - val_loss: 0.7764 - val_accuracy: 0.7311\n",
            "Epoch 00023: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f26d6205860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1QM3GsvXEva",
        "outputId": "168b90dc-62e7-488d-b192-e4cb0b433cf6"
      },
      "source": [
        "score = dense_model.evaluate(X_ws_valid_ft_mean_vectors, y_ws_valid_list, verbose = 0)\n",
        "print(\"Test score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score:  0.758577287197113\n",
            "Test accuracy:  0.7410622835159302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OokANjubGhyK"
      },
      "source": [
        "### Dense model with the minmax vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzuDhb0XHCis"
      },
      "source": [
        "Using the concatenation of the element-wise minimum and maximum of the wordvectors in each entry to get a vector (600 dimensional vectors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1xsN07YG0HK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7135a08-b9a2-4c82-f62e-cba6ec39d4dc"
      },
      "source": [
        "X_ws_train_ft_min_vectors = np.empty([len(X_ws_train_list), 300])\n",
        "for j in range(len(X_ws_train_list)):\n",
        "  X_ws_train_ft_vectors = np.empty([len(X_ws_train_list[j].split()), 300])\n",
        "  for i in range(len(X_ws_train_list[j].split())):\n",
        "    try:\n",
        "      X_ws_train_ft_vectors[i] = fasttext_vectors.wv[X_ws_train_list[j].split()[i]]\n",
        "    except KeyError:\n",
        "      X_ws_train_ft_vectors[i] = np.zeros(300)\n",
        "  X_ws_train_ft_min_vectors[j] = X_ws_train_ft_vectors.min(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF4NoU5YG0Q3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b9e6c5-b065-417d-9834-8b546ca10f71"
      },
      "source": [
        "X_ws_train_ft_max_vectors = np.empty([len(X_ws_train_list), 300])\n",
        "for j in range(len(X_ws_train_list)):\n",
        "  X_ws_train_ft_vectors = np.empty([len(X_ws_train_list[j].split()), 300])\n",
        "  for i in range(len(X_ws_train_list[j].split())):\n",
        "    try:\n",
        "      X_ws_train_ft_vectors[i] = fasttext_vectors.wv[X_ws_train_list[j].split()[i]]\n",
        "    except KeyError:\n",
        "      X_ws_train_ft_vectors[i] = np.zeros(300)\n",
        "  X_ws_train_ft_max_vectors[j] = X_ws_train_ft_vectors.max(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aeAz--TG0PN"
      },
      "source": [
        "X_ws_train_ft_minmax_vectors = np.concatenate((X_ws_train_ft_min_vectors, X_ws_train_ft_max_vectors), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5YXix25G0NQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb664d3-ef37-4ad0-c60e-4685ae4a4564"
      },
      "source": [
        "X_ws_train_ft_minmax_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13705, 600)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hep04co3G0E6"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X_ws_train_ft_minmax_vectors = scaler.fit_transform(X_ws_train_ft_minmax_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qYhzvaXHSJF"
      },
      "source": [
        "Doing the same for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usogV-3GHgRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4bf2a81-2299-4648-b2e0-d9b2a69e035b"
      },
      "source": [
        "X_ws_valid_ft_min_vectors = np.empty([len(X_ws_valid_list), 300])\n",
        "for j in range(len(X_ws_valid_list)):\n",
        "  X_ws_valid_ft_vectors = np.empty([len(X_ws_valid_list[j].split()), 300])\n",
        "  for i in range(len(X_ws_valid_list[j].split())):\n",
        "    try:\n",
        "      X_ws_valid_ft_vectors[i] = fasttext_vectors.wv[X_ws_valid_list[j].split()[i]]\n",
        "    except KeyError:\n",
        "      X_ws_valid_ft_vectors[i] = np.zeros(300)\n",
        "  X_ws_valid_ft_min_vectors[j] = X_ws_valid_ft_vectors.min(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PWXDl_yHgWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52036d23-be0e-42eb-80a1-336a4c730d2b"
      },
      "source": [
        "X_ws_valid_ft_max_vectors = np.empty([len(X_ws_valid_list), 300])\n",
        "for j in range(len(X_ws_valid_list)):\n",
        "  X_ws_valid_ft_vectors = np.empty([len(X_ws_valid_list[j].split()), 300])\n",
        "  for i in range(len(X_ws_valid_list[j].split())):\n",
        "    try:\n",
        "      X_ws_valid_ft_vectors[i] = fasttext_vectors.wv[X_ws_valid_list[j].split()[i]]\n",
        "    except KeyError:\n",
        "      X_ws_valid_ft_vectors[i] = np.zeros(300)\n",
        "  X_ws_valid_ft_max_vectors[j] = X_ws_valid_ft_vectors.min(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4WKZ9_RHgb3"
      },
      "source": [
        "X_ws_valid_ft_minmax_vectors = np.concatenate((X_ws_valid_ft_min_vectors, X_ws_valid_ft_max_vectors), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjm2p8mDHqB6"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X_ws_valid_ft_minmax_vectors = scaler.fit_transform(X_ws_valid_ft_minmax_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W72_SXoyGg9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0baad51-497d-4791-c751-2f36c73d7549"
      },
      "source": [
        "dense_model_mm = tf.keras.models.Sequential()\n",
        "dense_model_mm.add(tf.keras.Input(shape=(600,)))\n",
        "dense_model_mm.add(tf.keras.layers.Dense(300, activation='relu', kernel_initializer='he_normal'))\n",
        "dense_model_mm.add(tf.keras.layers.Dense(300, activation='relu', kernel_initializer='he_normal'))\n",
        "dense_model_mm.add(tf.keras.layers.Dense(300, activation='relu', kernel_initializer='he_normal'))\n",
        "dense_model_mm.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "dense_model_mm.output_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PkwmCd_GhKF"
      },
      "source": [
        "dense_model_mm.compile(optimizer=Adam(lr=1e-5),loss='categorical_crossentropy' ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyvJ4-bcGhHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e79816-9231-438b-ddb2-29202ee06ffb"
      },
      "source": [
        "dense_model_mm.fit(X_ws_train_ft_minmax_vectors, y_ws_train_list,  validation_split = 0.2,\n",
        "                     callbacks=[early_stopping], epochs=75)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 1.2236 - accuracy: 0.3841 - val_loss: 1.1237 - val_accuracy: 0.4279\n",
            "Epoch 2/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 1.0938 - accuracy: 0.4479 - val_loss: 1.0537 - val_accuracy: 0.4798\n",
            "Epoch 3/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 1.0214 - accuracy: 0.4930 - val_loss: 1.0106 - val_accuracy: 0.5086\n",
            "Epoch 4/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.9697 - accuracy: 0.5293 - val_loss: 0.9812 - val_accuracy: 0.5265\n",
            "Epoch 5/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.9290 - accuracy: 0.5586 - val_loss: 0.9563 - val_accuracy: 0.5469\n",
            "Epoch 6/75\n",
            "343/343 [==============================] - 2s 6ms/step - loss: 0.8944 - accuracy: 0.5838 - val_loss: 0.9384 - val_accuracy: 0.5556\n",
            "Epoch 7/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.8649 - accuracy: 0.6044 - val_loss: 0.9248 - val_accuracy: 0.5669\n",
            "Epoch 8/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.8386 - accuracy: 0.6200 - val_loss: 0.9090 - val_accuracy: 0.5768\n",
            "Epoch 9/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.8144 - accuracy: 0.6359 - val_loss: 0.8976 - val_accuracy: 0.5885\n",
            "Epoch 10/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.7920 - accuracy: 0.6526 - val_loss: 0.8886 - val_accuracy: 0.5932\n",
            "Epoch 11/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.7721 - accuracy: 0.6653 - val_loss: 0.8801 - val_accuracy: 0.5936\n",
            "Epoch 12/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.7526 - accuracy: 0.6758 - val_loss: 0.8706 - val_accuracy: 0.6053\n",
            "Epoch 13/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.7343 - accuracy: 0.6880 - val_loss: 0.8642 - val_accuracy: 0.6038\n",
            "Epoch 14/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.7170 - accuracy: 0.6981 - val_loss: 0.8590 - val_accuracy: 0.6100\n",
            "Epoch 15/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.7004 - accuracy: 0.7054 - val_loss: 0.8547 - val_accuracy: 0.6155\n",
            "Epoch 16/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.6847 - accuracy: 0.7140 - val_loss: 0.8496 - val_accuracy: 0.6180\n",
            "Epoch 17/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.6693 - accuracy: 0.7215 - val_loss: 0.8441 - val_accuracy: 0.6260\n",
            "Epoch 18/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.6548 - accuracy: 0.7277 - val_loss: 0.8427 - val_accuracy: 0.6235\n",
            "Epoch 19/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.6410 - accuracy: 0.7381 - val_loss: 0.8373 - val_accuracy: 0.6282\n",
            "Epoch 20/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.6267 - accuracy: 0.7446 - val_loss: 0.8361 - val_accuracy: 0.6250\n",
            "Epoch 21/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.6137 - accuracy: 0.7526 - val_loss: 0.8337 - val_accuracy: 0.6323\n",
            "Epoch 22/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.6008 - accuracy: 0.7567 - val_loss: 0.8293 - val_accuracy: 0.6341\n",
            "Epoch 23/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.5884 - accuracy: 0.7655 - val_loss: 0.8287 - val_accuracy: 0.6337\n",
            "Epoch 24/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.5763 - accuracy: 0.7724 - val_loss: 0.8281 - val_accuracy: 0.6348\n",
            "Epoch 25/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.5639 - accuracy: 0.7777 - val_loss: 0.8293 - val_accuracy: 0.6381\n",
            "Epoch 26/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.5526 - accuracy: 0.7842 - val_loss: 0.8244 - val_accuracy: 0.6392\n",
            "Epoch 27/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.5408 - accuracy: 0.7913 - val_loss: 0.8269 - val_accuracy: 0.6432\n",
            "Epoch 28/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.5302 - accuracy: 0.7962 - val_loss: 0.8254 - val_accuracy: 0.6370\n",
            "Epoch 29/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.5187 - accuracy: 0.8024 - val_loss: 0.8261 - val_accuracy: 0.6421\n",
            "Epoch 30/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.5082 - accuracy: 0.8066 - val_loss: 0.8280 - val_accuracy: 0.6403\n",
            "Epoch 31/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4979 - accuracy: 0.8150 - val_loss: 0.8267 - val_accuracy: 0.6359\n",
            "Epoch 32/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4870 - accuracy: 0.8193 - val_loss: 0.8268 - val_accuracy: 0.6425\n",
            "Epoch 33/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4767 - accuracy: 0.8263 - val_loss: 0.8293 - val_accuracy: 0.6432\n",
            "Epoch 34/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4666 - accuracy: 0.8317 - val_loss: 0.8270 - val_accuracy: 0.6428\n",
            "Epoch 35/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4565 - accuracy: 0.8378 - val_loss: 0.8270 - val_accuracy: 0.6476\n",
            "Epoch 36/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4466 - accuracy: 0.8427 - val_loss: 0.8292 - val_accuracy: 0.6468\n",
            "Epoch 37/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4369 - accuracy: 0.8477 - val_loss: 0.8301 - val_accuracy: 0.6454\n",
            "Epoch 38/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4273 - accuracy: 0.8511 - val_loss: 0.8342 - val_accuracy: 0.6476\n",
            "Epoch 39/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4176 - accuracy: 0.8577 - val_loss: 0.8330 - val_accuracy: 0.6468\n",
            "Epoch 40/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.4080 - accuracy: 0.8622 - val_loss: 0.8366 - val_accuracy: 0.6490\n",
            "Epoch 41/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3989 - accuracy: 0.8658 - val_loss: 0.8378 - val_accuracy: 0.6494\n",
            "Epoch 42/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3898 - accuracy: 0.8730 - val_loss: 0.8440 - val_accuracy: 0.6461\n",
            "Epoch 43/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3806 - accuracy: 0.8777 - val_loss: 0.8405 - val_accuracy: 0.6436\n",
            "Epoch 44/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3716 - accuracy: 0.8816 - val_loss: 0.8445 - val_accuracy: 0.6450\n",
            "Epoch 45/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3624 - accuracy: 0.8875 - val_loss: 0.8469 - val_accuracy: 0.6450\n",
            "Epoch 46/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3538 - accuracy: 0.8914 - val_loss: 0.8501 - val_accuracy: 0.6476\n",
            "Epoch 47/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3447 - accuracy: 0.8950 - val_loss: 0.8545 - val_accuracy: 0.6476\n",
            "Epoch 48/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3361 - accuracy: 0.9004 - val_loss: 0.8554 - val_accuracy: 0.6465\n",
            "Epoch 49/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3277 - accuracy: 0.9035 - val_loss: 0.8593 - val_accuracy: 0.6516\n",
            "Epoch 50/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3187 - accuracy: 0.9087 - val_loss: 0.8625 - val_accuracy: 0.6443\n",
            "Epoch 51/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3104 - accuracy: 0.9124 - val_loss: 0.8660 - val_accuracy: 0.6472\n",
            "Epoch 52/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.3022 - accuracy: 0.9180 - val_loss: 0.8718 - val_accuracy: 0.6447\n",
            "Epoch 53/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.2938 - accuracy: 0.9216 - val_loss: 0.8750 - val_accuracy: 0.6454\n",
            "Epoch 54/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.2856 - accuracy: 0.9259 - val_loss: 0.8773 - val_accuracy: 0.6443\n",
            "Epoch 55/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.2775 - accuracy: 0.9297 - val_loss: 0.8820 - val_accuracy: 0.6443\n",
            "Epoch 56/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.2696 - accuracy: 0.9347 - val_loss: 0.8887 - val_accuracy: 0.6428\n",
            "Epoch 57/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.2616 - accuracy: 0.9361 - val_loss: 0.8915 - val_accuracy: 0.6432\n",
            "Epoch 58/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.2537 - accuracy: 0.9409 - val_loss: 0.8966 - val_accuracy: 0.6406\n",
            "Epoch 59/75\n",
            "343/343 [==============================] - 2s 5ms/step - loss: 0.2460 - accuracy: 0.9438 - val_loss: 0.9040 - val_accuracy: 0.6370\n",
            "Epoch 00059: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f26d3210668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct0UXqPTGhD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24aa46f-141d-491e-ebec-ab3d8c6e0f7b"
      },
      "source": [
        "score = dense_model_mm.evaluate(X_ws_valid_ft_minmax_vectors, y_ws_valid_list, verbose = 0)\n",
        "print(\"Test score: \", score[0])\n",
        "print(\"Test accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score:  1.4225778579711914\n",
            "Test accuracy:  0.4877426028251648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7w6dlcfHZgM"
      },
      "source": [
        "### CNN model by Dani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zydwd_bOlDV"
      },
      "source": [
        "Only using sentences containing 100 or fewer words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHLWyHfLErES"
      },
      "source": [
        "cutoff = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvsvVopxC4R4"
      },
      "source": [
        "def shorten(X_input_sentences, y_input_labels, length):\n",
        "    X_out = []\n",
        "    y_out = []\n",
        "    for i in range(len(X_input_sentences)):\n",
        "        if len(X_input_sentences[i].split(' ')) <= length:\n",
        "            X_out.append(X_input_sentences[i])\n",
        "            y_out.append(y_input_labels[i])\n",
        "    return X_out, np.array(y_out)\n",
        "\n",
        "X_ws_train_list_short, y_ws_train_list_short = shorten(X_ws_train_list,y_ws_train_list,cutoff)\n",
        "X_ws_valid_list_short, y_ws_valid_list_short = shorten(X_ws_valid_list,y_ws_valid_list,cutoff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkG8ASonO45r"
      },
      "source": [
        "Filling out 100-long senteces by repeating the words in the sentence, e.g. for sentence \"We have talked.\" we get: [we have talked we have talked we .... have talked we]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92HvpvPSB1dN",
        "outputId": "390418d3-a41b-4fe8-a814-bdd75754975d"
      },
      "source": [
        "def set_of_sentences_to_vec(orig_list, vec_length, cutoff):\n",
        "\n",
        "    out = np.empty([len(orig_list), cutoff, vec_length])\n",
        "\n",
        "    for j in range(len(orig_list)):\n",
        "        add_vectors = np.empty([cutoff, vec_length])\n",
        "        \n",
        "        sentence_length = len(orig_list[j].split())\n",
        "        \n",
        "        for i in range(cutoff):\n",
        "            try:\n",
        "                add_vectors[i] = fasttext_vectors.wv[orig_list[j].split()[i % sentence_length]]\n",
        "            except KeyError:\n",
        "                add_vectors[i] = np.zeros(vec_length)\n",
        "\n",
        "        out[j] = add_vectors\n",
        "  \n",
        "    return out\n",
        "\n",
        "X_ws_train_ft_all_vectors = set_of_sentences_to_vec(X_ws_train_list_short, 300, cutoff)\n",
        "X_ws_valid_ft_all_vectors = set_of_sentences_to_vec(X_ws_valid_list_short, 300, cutoff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\AMD\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84BzSy5-CaOc",
        "outputId": "694d2f3e-53ea-41ab-be19-ebb17fb16f9b"
      },
      "source": [
        "X_ws_train_ft_all_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13640, 100, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 351
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDWE4PVFGjDY",
        "outputId": "ee9a7929-85b7-4569-ec84-b827f2b09a1f"
      },
      "source": [
        "X_ws_valid_ft_all_vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5850, 100, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 352
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUkKTwFc9qsA"
      },
      "source": [
        "X_ws_train_ft_all_vectors_tp = np.transpose(X_ws_train_ft_all_vectors,axes=(0,2,1))\n",
        "X_ws_valid_ft_all_vectors_tp = np.transpose(X_ws_valid_ft_all_vectors,axes=(0,2,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb_j1GM39qsA",
        "outputId": "93fcc822-bf3b-4f5d-9a80-e1bfd8b2908d"
      },
      "source": [
        "X_ws_train_ft_all_vectors_tp.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13640, 300, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 354
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsxOmOLK9qsA",
        "outputId": "859dba77-aa3a-4b80-b6d5-c11cf2dd5910"
      },
      "source": [
        "X_ws_valid_ft_all_vectors_tp.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5850, 300, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 355
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8QxqAA0PQ-O"
      },
      "source": [
        "Simple convolutional model with kernel size 7 that goes through the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28vZEmPEp8IH",
        "outputId": "dbc9078c-24ee-4b64-caea-40c8b758e698"
      },
      "source": [
        "conv_model = tf.keras.models.Sequential()\n",
        "conv_model.add(tf.keras.layers.Conv1D(300,kernel_size=7,\n",
        "        input_shape=(100,300), activation='relu',\n",
        "        kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3),\n",
        "        bias_regularizer=regularizers.l2(1e-3),\n",
        "        activity_regularizer=regularizers.l2(1e-4)))\n",
        "conv_model.add(tf.keras.layers.Flatten())\n",
        "conv_model.add(tf.keras.layers.Dense(94, activation='relu',\n",
        "        kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3),\n",
        "        bias_regularizer=regularizers.l2(1e-3),\n",
        "        activity_regularizer=regularizers.l2(1e-4)))\n",
        "conv_model.add(tf.keras.layers.Dropout(0.4))\n",
        "conv_model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "conv_model.output_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 361
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHhSoQ-k9qsA",
        "outputId": "840221ec-e2ef-4154-fc07-8c549f0a2537"
      },
      "source": [
        "conv_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_169 (Conv1D)          (None, 94, 300)           630300    \n",
            "_________________________________________________________________\n",
            "flatten_51 (Flatten)         (None, 28200)             0         \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 64)                1804864   \n",
            "_________________________________________________________________\n",
            "dropout_66 (Dropout)         (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_122 (Dense)            (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 2,435,359\n",
            "Trainable params: 2,435,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CfTvYd09qsA"
      },
      "source": [
        "conv_model.compile(optimizer='adam',loss='categorical_crossentropy' ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gAlbuf9Pghm"
      },
      "source": [
        "More complicated convolutional model that uses 4 different kernel sizes, then concatenates them and finishes with a single Dropout and Dense layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFjVa9RE9qsA",
        "outputId": "d5a287d2-0e8d-49ac-8a74-c7ffc9200825"
      },
      "source": [
        "inp = tf.keras.Input(shape=(300,100), name='input')\n",
        "\n",
        "def conv_pool_layer(kernel_size,pool_size,input_layer,name):\n",
        "    \n",
        "    conv = tf.keras.layers.Conv1D(100,kernel_size=kernel_size,\n",
        "        input_shape=(300,100), activation='relu', padding='same',\n",
        "        kernel_regularizer=regularizers.l1_l2(l1=1e-4, l2=1e-3),\n",
        "        bias_regularizer=regularizers.l2(1e-3),\n",
        "        activity_regularizer=regularizers.l2(1e-4),\n",
        "        data_format='channels_last', name='1D_conv' + name)(input_layer)\n",
        "    \n",
        "    pool = max_pool3 = tf.keras.layers.MaxPooling1D(pool_size=pool_size,\n",
        "        data_format='channels_first', name= '1D_maxpool' + name)(conv)\n",
        "\n",
        "    return pool\n",
        "\n",
        "model3 = tf.keras.Model(inputs=inp, outputs=conv_pool_layer(3,2,inp,'3'))\n",
        "model5 = tf.keras.Model(inputs=inp, outputs=conv_pool_layer(5,2,inp,'5'))\n",
        "model7 = tf.keras.Model(inputs=inp, outputs=conv_pool_layer(7,4,inp,'7'))\n",
        "model9 = tf.keras.Model(inputs=inp, outputs=conv_pool_layer(9,4,inp,'9'))\n",
        "\n",
        "combined = tf.keras.layers.concatenate([model3.output, model5.output,\n",
        "                                        model7.output, model9.output], name='concatenate')\n",
        "\n",
        "flat = tf.keras.layers.Flatten(name='flatten')(combined)\n",
        "drop = tf.keras.layers.Dropout(0.57,name='droput_0.5')(flat)\n",
        "dense = tf.keras.layers.Dense(3, activation='softmax',name='dense')(drop)\n",
        "\n",
        "conv_model2 = tf.keras.Model(inputs=[model3.input],\n",
        "                            outputs=dense,name='Multiconv_Model')\n",
        "conv_model2.output_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 375
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qONLMQbvM-o6",
        "outputId": "21101936-d853-47b3-deda-7645e8fcdeb6"
      },
      "source": [
        "conv_model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Multiconv_Model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 300, 100)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "1D_conv3 (Conv1D)               (None, 300, 100)     30100       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "1D_conv5 (Conv1D)               (None, 300, 100)     50100       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "1D_conv7 (Conv1D)               (None, 300, 100)     70100       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "1D_conv9 (Conv1D)               (None, 300, 100)     90100       input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "1D_maxpool3 (MaxPooling1D)      (None, 300, 50)      0           1D_conv3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "1D_maxpool5 (MaxPooling1D)      (None, 300, 50)      0           1D_conv5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "1D_maxpool7 (MaxPooling1D)      (None, 300, 25)      0           1D_conv7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "1D_maxpool9 (MaxPooling1D)      (None, 300, 25)      0           1D_conv9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 300, 150)     0           1D_maxpool3[0][0]                \n",
            "                                                                 1D_maxpool5[0][0]                \n",
            "                                                                 1D_maxpool7[0][0]                \n",
            "                                                                 1D_maxpool9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 45000)        0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "droput_0.5 (Dropout)            (None, 45000)        0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 3)            135003      droput_0.5[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 375,403\n",
            "Trainable params: 375,403\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7yxL0avOSBi"
      },
      "source": [
        "conv_model2.compile(optimizer='adam',loss='categorical_crossentropy' ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "n3TQ7acB9qsA",
        "outputId": "d1b52e8a-1e45-44f8-a841-a9373cfa5087"
      },
      "source": [
        "for i in range(8):\n",
        "    print(\"SESSION \" + str(i)) \n",
        "    conv_model.fit(X_ws_train_ft_all_vectors, y_ws_train_list_short,\n",
        "                   epochs=5, batch_size=20, validation_split=1/11)\n",
        "    conv_model.save('author_identification_conv2_model' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SESSION 0\n",
            "Train on 12400 samples, validate on 1240 samples\n",
            "Epoch 1/5\n",
            "  320/12400 [..............................] - ETA: 9:02 - loss: 1.0007 - acc: 0.5656"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-372-bc4f178c946f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SESSION \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     conv_model.fit(X_ws_train_ft_all_vectors, y_ws_train_list_short,\n\u001b[1;32m----> 4\u001b[1;33m                    epochs=5, batch_size=20, validation_split=1/11)\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mconv_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'author_identification_conv2_model'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQfPNUSFPoFZ"
      },
      "source": [
        "Teaching this Neural Net took ~8 hours for 30 epochs, so it has been interrupted multiple times and saved into a separate Keras H5 file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Tq6Pgcwj9qsB",
        "outputId": "272b9b0f-06a7-4fc5-a917-3441d8e388cf"
      },
      "source": [
        "for i in range(8):\n",
        "    print(\"SESSION \" + str(i)) \n",
        "    conv_model2.fit(X_ws_train_ft_all_vectors_tp, y_ws_train_list_short,\n",
        "                   epochs=5, batch_size=20, validation_split=1/11)\n",
        "    conv_model2.save('author_identification_multiconv2_model' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SESSION 0\n",
            "Train on 12400 samples, validate on 1240 samples\n",
            "Epoch 1/5\n",
            "12400/12400 [==============================] - 961s 77ms/sample - loss: 1.3436 - acc: 0.5388 - val_loss: 0.8882 - val_acc: 0.6621\n",
            "Epoch 2/5\n",
            "12400/12400 [==============================] - 940s 76ms/sample - loss: 0.8725 - acc: 0.6653 - val_loss: 0.8531 - val_acc: 0.6589\n",
            "Epoch 3/5\n",
            "12400/12400 [==============================] - 953s 77ms/sample - loss: 0.8173 - acc: 0.6938 - val_loss: 0.8310 - val_acc: 0.6710\n",
            "Epoch 4/5\n",
            "12400/12400 [==============================] - 929s 75ms/sample - loss: 0.7828 - acc: 0.7198 - val_loss: 0.8635 - val_acc: 0.6476\n",
            "Epoch 5/5\n",
            "12400/12400 [==============================] - 940s 76ms/sample - loss: 0.7582 - acc: 0.7296 - val_loss: 0.8605 - val_acc: 0.6815\n",
            "SESSION 1\n",
            "Train on 12400 samples, validate on 1240 samples\n",
            "Epoch 1/5\n",
            "12400/12400 [==============================] - 789s 64ms/sample - loss: 0.7487 - acc: 0.7435 - val_loss: 0.8902 - val_acc: 0.6935\n",
            "Epoch 2/5\n",
            "12400/12400 [==============================] - 777s 63ms/sample - loss: 0.7999 - acc: 0.7598 - val_loss: 0.9803 - val_acc: 0.6935\n",
            "Epoch 3/5\n",
            "12400/12400 [==============================] - 919s 74ms/sample - loss: 0.7960 - acc: 0.7787 - val_loss: 1.0038 - val_acc: 0.6645\n",
            "Epoch 4/5\n",
            "12400/12400 [==============================] - 959s 77ms/sample - loss: 0.7318 - acc: 0.7969 - val_loss: 0.9811 - val_acc: 0.6895\n",
            "Epoch 5/5\n",
            "12400/12400 [==============================] - 974s 79ms/sample - loss: 0.6613 - acc: 0.8139 - val_loss: 0.9660 - val_acc: 0.6823\n",
            "SESSION 2\n",
            "Train on 12400 samples, validate on 1240 samples\n",
            "Epoch 1/5\n",
            "  100/12400 [..............................] - ETA: 9:47 - loss: 0.5953 - acc: 0.8500"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-378-c7659a14b747>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SESSION \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     conv_model2.fit(X_ws_train_ft_all_vectors_tp, y_ws_train_list_short,\n\u001b[1;32m----> 4\u001b[1;33m                    epochs=5, batch_size=20, validation_split=1/11)\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mconv_model2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'author_identification_multiconv2_model'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYpVsu6N9qsB"
      },
      "source": [
        "conv_model2 = tf.keras.models.load_model('author_identification_multiconv2_model1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9ur8ZAAx9qsB",
        "outputId": "9529a21b-d8d9-493e-d7c1-96328af77a8c"
      },
      "source": [
        "for i in range(2,8):\n",
        "    print(\"SESSION \" + str(i)) \n",
        "    conv_model2.fit(X_ws_train_ft_all_vectors_tp, y_ws_train_list_short,\n",
        "                   epochs=5, batch_size=20, validation_split=1/11)\n",
        "    conv_model2.save('author_identification_multiconv2_model' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SESSION 2\n",
            "Train on 12400 samples, validate on 1240 samples\n",
            "Epoch 1/5\n",
            "12400/12400 [==============================] - 852s 69ms/sample - loss: 0.6209 - acc: 0.8281 - val_loss: 1.0151 - val_acc: 0.6806\n",
            "Epoch 2/5\n",
            "12400/12400 [==============================] - 964s 78ms/sample - loss: 0.6006 - acc: 0.8428 - val_loss: 0.9591 - val_acc: 0.6823\n",
            "Epoch 3/5\n",
            "12400/12400 [==============================] - 1042s 84ms/sample - loss: 0.5900 - acc: 0.8441 - val_loss: 0.9958 - val_acc: 0.6677\n",
            "Epoch 4/5\n",
            "12400/12400 [==============================] - 842s 68ms/sample - loss: 0.5723 - acc: 0.8552 - val_loss: 1.0274 - val_acc: 0.6766\n",
            "Epoch 5/5\n",
            "12400/12400 [==============================] - 823s 66ms/sample - loss: 0.5647 - acc: 0.8643 - val_loss: 1.0167 - val_acc: 0.6815\n",
            "SESSION 3\n",
            "Train on 12400 samples, validate on 1240 samples\n",
            "Epoch 1/5\n",
            "12400/12400 [==============================] - 812s 65ms/sample - loss: 0.5666 - acc: 0.8723 - val_loss: 1.1128 - val_acc: 0.6629\n",
            "Epoch 2/5\n",
            "12400/12400 [==============================] - 845s 68ms/sample - loss: 0.5593 - acc: 0.8802 - val_loss: 1.0915 - val_acc: 0.6694\n",
            "Epoch 3/5\n",
            "12400/12400 [==============================] - 831s 67ms/sample - loss: 0.5831 - acc: 0.8755 - val_loss: 1.1399 - val_acc: 0.6750\n",
            "Epoch 4/5\n",
            "12400/12400 [==============================] - 824s 66ms/sample - loss: 0.6072 - acc: 0.8793 - val_loss: 1.1883 - val_acc: 0.6637\n",
            "Epoch 5/5\n",
            "12400/12400 [==============================] - 984s 79ms/sample - loss: 0.5726 - acc: 0.8895 - val_loss: 1.2259 - val_acc: 0.6540\n",
            "SESSION 4\n",
            "Train on 12400 samples, validate on 1240 samples\n",
            "Epoch 1/5\n",
            "12400/12400 [==============================] - 1046s 84ms/sample - loss: 0.5593 - acc: 0.8916 - val_loss: 1.1514 - val_acc: 0.6758\n",
            "Epoch 2/5\n",
            "12400/12400 [==============================] - 1017s 82ms/sample - loss: 0.5164 - acc: 0.9025 - val_loss: 1.1577 - val_acc: 0.6847\n",
            "Epoch 3/5\n",
            "12400/12400 [==============================] - 1007s 81ms/sample - loss: 0.5121 - acc: 0.9035 - val_loss: 1.1557 - val_acc: 0.6669\n",
            "Epoch 4/5\n",
            "12400/12400 [==============================] - 957s 77ms/sample - loss: 0.4780 - acc: 0.9110 - val_loss: 1.1818 - val_acc: 0.6750\n",
            "Epoch 5/5\n",
            "12400/12400 [==============================] - 898s 72ms/sample - loss: 0.4788 - acc: 0.9092 - val_loss: 1.1616 - val_acc: 0.6677\n",
            "SESSION 5\n",
            "Train on 12400 samples, validate on 1240 samples\n",
            "Epoch 1/5\n",
            "12400/12400 [==============================] - 924s 75ms/sample - loss: 0.4831 - acc: 0.9094 - val_loss: 1.1818 - val_acc: 0.6669\n",
            "Epoch 2/5\n",
            "12400/12400 [==============================] - 935s 75ms/sample - loss: 0.5066 - acc: 0.9090 - val_loss: 1.2256 - val_acc: 0.6702\n",
            "Epoch 3/5\n",
            "12400/12400 [==============================] - 1011s 81ms/sample - loss: 0.4963 - acc: 0.9160 - val_loss: 1.2544 - val_acc: 0.6621\n",
            "Epoch 4/5\n",
            "12400/12400 [==============================] - 1364s 110ms/sample - loss: 0.5138 - acc: 0.9106 - val_loss: 1.2414 - val_acc: 0.6734\n",
            "Epoch 5/5\n",
            "12400/12400 [==============================] - 1286s 104ms/sample - loss: 0.5067 - acc: 0.9160 - val_loss: 1.2362 - val_acc: 0.6613\n",
            "SESSION 6\n",
            "Train on 12400 samples, validate on 1240 samples\n",
            "Epoch 1/5\n",
            " 5760/12400 [============>.................] - ETA: 11:44 - loss: 0.4576 - acc: 0.9323"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-389-53b81ba02d0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SESSION \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     conv_model2.fit(X_ws_train_ft_all_vectors_tp, y_ws_train_list_short,\n\u001b[1;32m----> 4\u001b[1;33m                    epochs=5, batch_size=20, validation_split=1/11)\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mconv_model2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'author_identification_multiconv2_model'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxWXW4i79qsB"
      },
      "source": [
        "conv_model2 = tf.keras.models.load_model('author_identification_multiconv2_model5.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRKrTrQJ9qsB"
      },
      "source": [
        "with open('X_ws_train_ft_all_vectors.npy', 'wb') as f:\n",
        "    np.save(f, X_ws_train_ft_all_vectors)\n",
        "with open('X_ws_train_ft_all_vectors_tp.npy', 'wb') as f:\n",
        "    np.save(f, X_ws_train_ft_all_vectors_tp)\n",
        "with open('y_ws_train_list_short.npy', 'wb') as f:\n",
        "    np.save(f, y_ws_train_list_short)\n",
        "with open('X_ws_valid_ft_all_vectors.npy', 'wb') as f:\n",
        "    np.save(f, X_ws_valid_ft_all_vectors)\n",
        "with open('X_ws_valid_ft_all_vectors_tp.npy', 'wb') as f:\n",
        "    np.save(f, X_ws_valid_ft_all_vectors_tp)\n",
        "with open('y_ws_valid_list_short.npy', 'wb') as f:\n",
        "    np.save(f, y_ws_valid_list_short)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFO1q7JF9qsB"
      },
      "source": [
        "'''\n",
        "with open('X_ws_train_ft_all_vectors.npy', 'rb') as f:\n",
        "    X_ws_train_ft_all_vectors = np.load(f)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jceSy5Vr9qsB",
        "outputId": "d656270a-f904-411d-85d8-a0cc746a75cd"
      },
      "source": [
        "conv_model.evaluate(X_ws_valid_ft_all_vectors, y_ws_valid_list_short)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5850/5850 [==============================] - 172s 29ms/sample - loss: 1.0111 - acc: 0.5489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0110511044151762, 0.54888886]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 365
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh8ctIn99qsB",
        "outputId": "ae868fd4-5649-4b8e-8a7b-3c98ab3137b4"
      },
      "source": [
        "conv_model2.evaluate(X_ws_valid_ft_all_vectors_tp, y_ws_valid_list_short)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5850/5850 [==============================] - 197s 34ms/sample - loss: 1.2288 - acc: 0.6598\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2288284897192931, 0.6598291]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 391
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PrSHkho9qsB"
      },
      "source": [
        "saved_model = tf.keras.models.load_model('author_identification_conv_model5.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8569OLk9qsB",
        "outputId": "d6f9ce2e-e6d9-445d-a5fd-5af60b075a2e"
      },
      "source": [
        "saved_model.evaluate(X_ws_valid_ft_all_vectors, y_ws_valid_list_short)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5850/5850 [==============================] - 131s 22ms/sample - loss: 1.5943 - acc: 0.6821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5943330551620223, 0.6820513]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y_TlXpF9qsC"
      },
      "source": [
        "saved_model2 = tf.keras.models.load_model('author_identification_multiconv2_model1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeMtCqtoP8NV"
      },
      "source": [
        "Unfortunately the above model didn't give as good of an accuracy as the simple Dense model, even through we've tried many parameter settings:\n",
        "- Chaning the number of Convolutional Layers and how they connect\n",
        "- Adding or removing Dense Layers at the end\n",
        "- Changing the Dropout rate\n",
        "- Adding regularization\n",
        "- Changing the structure of the Neural Net.\n",
        "\n",
        "We'll do some hyperparameter-optimization to get a better accuracy later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V6N4vgn9qsC",
        "outputId": "4f4939ad-efff-4b55-bc05-7681a985a769"
      },
      "source": [
        "saved_model2.evaluate(X_ws_valid_ft_all_vectors_tp, y_ws_valid_list_short)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5850/5850 [==============================] - 181s 31ms/sample - loss: 0.9471 - acc: 0.6858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9470657617821653, 0.68581194]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 380
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPFG3WC69qsC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}