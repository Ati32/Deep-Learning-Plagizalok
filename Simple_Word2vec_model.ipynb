{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple_Word2vec_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81jwKzoOEJSM"
      },
      "source": [
        "# Simple Word2vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R45xbP2sEOe6"
      },
      "source": [
        "Imports and Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4cYv7iGL70K",
        "outputId": "55f19706-1c16-4b48-d992-919183c5dff4"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import pickle\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "random_state = 7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZDZEeviL-Bp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e913b011-c9e3-4c9f-b48e-c7e0ec35285b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train = pd.read_csv('/content/drive/My Drive/Author_identification/train.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/Author_identification/test.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "9wMnQrp5MOUi",
        "outputId": "a44b6dca-836f-497c-8989-69520a7dde09"
      },
      "source": [
        "train['sentences'] = train.text.transform(lambda x: len(sent_tokenize(x)))\n",
        "train['words'] = train.text.transform(lambda x: len(word_tokenize(x)))\n",
        "train['text_length'] = train.text.transform(lambda x: len(x))\n",
        "\n",
        "text_info = train.groupby(\"author\")[['sentences','words','text_length']].sum()\n",
        "text_info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>words</th>\n",
              "      <th>text_length</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>author</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>EAP</th>\n",
              "      <td>8206</td>\n",
              "      <td>232184</td>\n",
              "      <td>1123585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HPL</th>\n",
              "      <td>5876</td>\n",
              "      <td>173979</td>\n",
              "      <td>878178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MWS</th>\n",
              "      <td>6128</td>\n",
              "      <td>188824</td>\n",
              "      <td>916632</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        sentences   words  text_length\n",
              "author                                \n",
              "EAP          8206  232184      1123585\n",
              "HPL          5876  173979       878178\n",
              "MWS          6128  188824       916632"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg4aCFDbMZTZ"
      },
      "source": [
        "author_dict = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n",
        "new_author = []\n",
        "new_text_ws = [] # with stopwords\n",
        "for i, row in train[['text','author']].iterrows():\n",
        "    word_tokens = word_tokenize(row['text'])\n",
        "    new_text_ws.append(\" \".join([w for w in word_tokens if not w in [',','.','?','!',':',';',\"'\",'\"','-',\"''\",\"`\",\"``\"]]).lower())\n",
        "    new_author.append(author_dict[row['author']])\n",
        "new_train_ws = pd.DataFrame(data={'text': new_text_ws, 'author': new_author})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INFDGwRwQeO7"
      },
      "source": [
        "Word2vec vectors for each author"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StgQqZImZoEJ"
      },
      "source": [
        "#X_train, X_valid = train_test_split(data_0, test_size=0.3, random_state=random_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFX1QzftJ4bB"
      },
      "source": [
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "data_0 = new_train_ws.loc[new_train_ws['author'] == 0]\n",
        "data_1 = new_train_ws.loc[new_train_ws['author'] == 1]\n",
        "data_2 = new_train_ws.loc[new_train_ws['author'] == 2]\n",
        "\n",
        "data_0 = data_0.sample(frac=1).reset_index(drop=True)\n",
        "data_1 = data_1.sample(frac=1).reset_index(drop=True)\n",
        "data_2 = data_2.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "train_0, test_0 = train_test_split(data_0, test_size=0.1, random_state=random_state)\n",
        "train_1, test_1 = train_test_split(data_1, test_size=0.1, random_state=random_state)\n",
        "train_2, test_2 = train_test_split(data_2, test_size=0.1, random_state=random_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R8Xg5NlcwuY"
      },
      "source": [
        "min_count = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-whc2knKDIc",
        "outputId": "8ba057c2-14d8-4ce5-b921-8dbb64d11dbf"
      },
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "sent_0 = [row.split() for row in train_0['text']]\n",
        "phrases_0 = Phrases(sent_0, min_count=min_count, progress_per=10000)\n",
        "bigram_0 = Phraser(phrases_0)\n",
        "sentences_0 = bigram_0[sent_0]\n",
        "w2v_model_0 = Word2Vec(min_count=min_count, window=2, size=300, sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20, workers=2)\n",
        "\n",
        "from time import time\n",
        "t = time()\n",
        "w2v_model_0.build_vocab(sentences_0, progress_per=10000)\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "t = time()\n",
        "w2v_model_0.train(sentences_0, total_examples=w2v_model_0.corpus_count, epochs=30, report_delay=1)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "w2v_model_0.init_sims(replace=True)\n",
        "\n",
        "w2v_model_0.wv.similarity('you', 'are')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to build vocab: 0.02 mins\n",
            "Time to train the model: 0.4 mins\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9255816"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moeXGhNoKDFg",
        "outputId": "66159ce8-1ce9-4b9f-a239-f8f3177610a4"
      },
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "sent_1 = [row.split() for row in train_1['text']]\n",
        "phrases_1 = Phrases(sent_1, min_count=min_count, progress_per=10000)\n",
        "bigram_1 = Phraser(phrases_1)\n",
        "sentences_1 = bigram_1[sent_1]\n",
        "w2v_model_1 = Word2Vec(min_count=min_count, window=2, size=300, sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20, workers=2)\n",
        "\n",
        "from time import time\n",
        "t = time()\n",
        "w2v_model_1.build_vocab(sentences_1, progress_per=10000)\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "t = time()\n",
        "w2v_model_1.train(sentences_1, total_examples=w2v_model_1.corpus_count, epochs=30, report_delay=1)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "w2v_model_1.init_sims(replace=True)\n",
        "\n",
        "w2v_model_1.wv.similarity('you', 'are')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to build vocab: 0.02 mins\n",
            "Time to train the model: 0.3 mins\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9961437"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPUnAsnUKF7V",
        "outputId": "d769ac4f-cee2-4ede-b756-356731e016e2"
      },
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "sent_2 = [row.split() for row in train_2['text']]\n",
        "phrases_2 = Phrases(sent_2, min_count=min_count, progress_per=10000)\n",
        "bigram_2 = Phraser(phrases_2)\n",
        "sentences_2 = bigram_2[sent_2]\n",
        "w2v_model_2 = Word2Vec(min_count=min_count, window=2, size=300, sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20, workers=2)\n",
        "\n",
        "from time import time\n",
        "t = time()\n",
        "w2v_model_2.build_vocab(sentences_2, progress_per=10000)\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "t = time()\n",
        "w2v_model_2.train(sentences_2, total_examples=w2v_model_2.corpus_count, epochs=30, report_delay=1)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "w2v_model_2.init_sims(replace=True)\n",
        "\n",
        "w2v_model_2.wv.similarity('you', 'are')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to build vocab: 0.02 mins\n",
            "Time to train the model: 0.32 mins\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9804925"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb9xnUtQKF2C"
      },
      "source": [
        "word_vectors_0 = w2v_model_0.wv\n",
        "word_vectors_1 = w2v_model_1.wv\n",
        "word_vectors_2 = w2v_model_2.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teBbA2ydQsOF"
      },
      "source": [
        "Testing function: \n",
        "- input: test sentence\n",
        "- output: the most likely author"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73p74XVbKKJE"
      },
      "source": [
        "def testing(X,y):\n",
        "  true = 0\n",
        "  false = 0\n",
        "  for index, a in X.items():\n",
        "    #print(a)\n",
        "\n",
        "    sims = []\n",
        "    tokens = word_tokenize(a)\n",
        "    lenght = len(tokens)\n",
        "\n",
        "    sim_0 = 0.5*(lenght-1)\n",
        "    sim_1 = 0.5*(lenght-1)\n",
        "    sim_2 = 0.5*(lenght-1)\n",
        "    for i in range(lenght-1):\n",
        "      #print(i)\n",
        "      if tokens[i] in word_vectors_0 and tokens[i+1] in word_vectors_0:\n",
        "        sim = w2v_model_0.wv.similarity(tokens[i], tokens[i+1])\n",
        "        sim_0 += sim-0.5   \n",
        "\n",
        "      if tokens[i] in word_vectors_1 and tokens[i+1] in word_vectors_1:\n",
        "        sim = w2v_model_1.wv.similarity(tokens[i], tokens[i+1])\n",
        "        sim_1 += sim-0.5\n",
        "\n",
        "      if tokens[i] in word_vectors_2 and tokens[i+1] in word_vectors_2:\n",
        "        sim = w2v_model_2.wv.similarity(tokens[i], tokens[i+1])\n",
        "        sim_2 += sim-0.5\n",
        "\n",
        "    #print((sim_0,sim_1,sim_2))\n",
        "    sims.append(sim_0/(lenght-1))\n",
        "    sims.append(sim_1/(lenght-1))\n",
        "    sims.append(sim_2/(lenght-1))\n",
        "    \n",
        "    #print(sims)\n",
        "    #print(np.argmax(sims),y[index])\n",
        "    if np.argmax(sims) == y[index]:\n",
        "      true += 1\n",
        "    else:\n",
        "      false += 1\n",
        "  print(true/(true+false))\n",
        "  return (true, false)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkSIjKJWKKG1",
        "outputId": "9602f480-0ac7-42f5-817d-054b2796f6bd"
      },
      "source": [
        "true_0, false_0 = testing(test_0['text'],test_0['author'])\n",
        "true_1, false_1 = testing(test_1['text'],test_1['author'])\n",
        "true_2, false_2 = testing(test_2['text'],test_2['author'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5088607594936709\n",
            "0.7783687943262412\n",
            "0.7487603305785124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0QY6apfKKEP",
        "outputId": "55dedf30-3ca7-44cc-cf43-4d9c914f0ccc"
      },
      "source": [
        "true = true_0+ true_1 +true_2\n",
        "false = false_0 + false_1 + false_2\n",
        "print(true/(true+false))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6605410923940787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwg-Hl8SErOT"
      },
      "source": [
        "So, this method giving an accuracy of 66.05%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-P2bX3rfWtn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}